{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19bbc8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import site\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from math import ceil\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "import transformers\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    LlamaTokenizer,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35917096-f03c-468e-9410-308ce438ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d396492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e8d9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1867685f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local model path is: /scratch/umeleti/code/LLM/Text2SQL/MODEL_CACHE/NousResearch--CodeLlama-7b-hf\n"
     ]
    }
   ],
   "source": [
    "local_model_id = config.BASE_MODEL.replace(\"/\", \"--\")\n",
    "local_model_path = os.path.join(config.MODEL_CACHE_PATH, local_model_id)\n",
    "print(f\"local model path is: {local_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2341c33-edb0-4d01-9e74-d31ee2ba2bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find finetuning.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/umeleti/.netrc\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = 'finetuning'\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = f\"text-to-sql-finetune-model-name_{config.BASE_MODEL.replace('/', '_')}\"\n",
    "wandb.login()\n",
    "\n",
    "ENABLE_WANDB = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3131bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.41s/it]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            local_model_path,\n",
    "            quantization_config=quantization_config)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ef75df6-0781-44df-a521-58647c3b925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_sql(input_question, context, output=\"\"):\n",
    "    \"\"\"\n",
    "    Generates a prompt for fine-tuning the LLM model for text-to-SQL tasks.\n",
    "\n",
    "    Parameters:\n",
    "        input_question (str): The input text or question to be converted to SQL.\n",
    "        context (str): The schema or context in which the SQL query operates.\n",
    "        output (str, optional): The expected SQL query as the output.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string serving as the prompt for the fine-tuning task.\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "\n",
    "### Input:\n",
    "{input_question}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef8b3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_CONFIG = LoraConfig(\n",
    "    r=16,  # rank\n",
    "    lora_alpha=32,  # scaling factor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"], \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, LORA_CONFIG).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6d719ad8-4555-4af5-ac8f-3ac53186f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(local_model_path)\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7657fb0e-5b58-4598-94eb-64a65edc7d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"b-mc2/sql-create-context\"\n",
    "MODEL_PATH = \"./final_model\"\n",
    "ADAPTER_PATH = \"./lora_adapters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "056f8ea1-70ae-466d-804f-caf65e60eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bab3933-b29e-4d28-82af-871140a0886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data_points, add_eos_token=True, train_on_inputs=False, cutoff_len=512):\n",
    "        question = data_points[\"question\"]\n",
    "        context = data_points[\"context\"]\n",
    "        answer = data_points[\"answer\"]\n",
    "        combined_text = generate_prompt_sql(question, context, answer)\n",
    "        tokenized = tokenizer(\n",
    "                combined_text,\n",
    "                truncation=True,\n",
    "                max_length=cutoff_len,\n",
    "                padding=False,\n",
    "                return_tensors=None)\n",
    "\n",
    "        if (tokenized[\"input_ids\"][-1] != tokenizer.eos_token_id and add_eos_token\n",
    "                and len(tokenized[\"input_ids\"]) < cutoff_len):\n",
    "                tokenized[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "                tokenized[\"attention_mask\"].append(1)\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "        return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0d9f261-38f1-4feb-85d9-a61cb6f0de0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fade23cd-050c-480b-b494-acf9e9d1eae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78477/78477 [00:47<00:00, 1642.79 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1506.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val_set_size = 100\n",
    "train_val_split = data[\"train\"].train_test_split(\n",
    "                test_size=val_set_size, shuffle=True, seed=42\n",
    "            )\n",
    "train_data = train_val_split[\"train\"].shuffle().map(tokenize_data)\n",
    "val_data = train_val_split[\"test\"].shuffle().map(tokenize_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5cc2be17-8155-428d-add2-9692a58e1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_batch_size=4\n",
    "warmup_steps=20\n",
    "learning_rate=2e-5\n",
    "max_steps=200\n",
    "gradient_accum_steps=4\n",
    "save_steps = 20\n",
    "eval_steps = 20\n",
    "max_grad_norm = 0.3\n",
    "save_total_limit = 3\n",
    "logging_steps = 20\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=per_device_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accum_steps,\n",
    "            warmup_steps=warmup_steps,\n",
    "            save_steps=save_steps,\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=eval_steps,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            #max_grad_norm=max_grad_norm,\n",
    "            bf16=True,\n",
    "            #lr_scheduler_type=\"cosine\",\n",
    "            load_best_model_at_end=True,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            group_by_length=True,\n",
    "            save_total_limit=save_total_limit,\n",
    "            logging_steps=logging_steps,\n",
    "            optim=\"adamw_hf\",\n",
    "            output_dir=\"./lora_adapters\",\n",
    "            logging_dir=\"./logs\",\n",
    "            report_to=\"wandb\" if ENABLE_WANDB else [],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "99418119-ae91-4a6a-86f3-d87e80949ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "                model=model,\n",
    "                train_dataset=train_data,\n",
    "                eval_dataset=val_data,\n",
    "                args=training_args,\n",
    "                data_collator=DataCollatorForSeq2Seq(\n",
    "                    tokenizer,\n",
    "                    pad_to_multiple_of=8,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                ),\n",
    "            )\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2e888530-fe51-4a11-8299-7c7171e34a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "31c101df-82a8-47fb-8741-6b2c6b7eb89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umeleti/.local/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3016, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'eval_loss': 2.350579261779785, 'eval_runtime': 3.1337, 'eval_samples_per_second': 31.911, 'eval_steps_per_second': 4.148, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umeleti/.conda/envs/pytorch-p100/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /scratch/umeleti/code/LLM/Text2SQL/MODEL_CACHE/NousResearch--CodeLlama-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1803, 'learning_rate': 1.7777777777777777e-05, 'epoch': 0.01}\n",
      "{'eval_loss': 1.9797790050506592, 'eval_runtime': 3.1496, 'eval_samples_per_second': 31.75, 'eval_steps_per_second': 4.127, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umeleti/.conda/envs/pytorch-p100/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /scratch/umeleti/code/LLM/Text2SQL/MODEL_CACHE/NousResearch--CodeLlama-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7741, 'learning_rate': 1.555555555555556e-05, 'epoch': 0.01}\n",
      "{'eval_loss': 1.608078956604004, 'eval_runtime': 3.1587, 'eval_samples_per_second': 31.659, 'eval_steps_per_second': 4.116, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umeleti/.conda/envs/pytorch-p100/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /scratch/umeleti/code/LLM/Text2SQL/MODEL_CACHE/NousResearch--CodeLlama-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4055, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.02}\n",
      "{'eval_loss': 1.253481149673462, 'eval_runtime': 3.1671, 'eval_samples_per_second': 31.575, 'eval_steps_per_second': 4.105, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umeleti/.conda/envs/pytorch-p100/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /scratch/umeleti/code/LLM/Text2SQL/MODEL_CACHE/NousResearch--CodeLlama-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0596, 'learning_rate': 1.1111111111111113e-05, 'epoch': 0.02}\n",
      "{'eval_loss': 1.1013339757919312, 'eval_runtime': 3.1697, 'eval_samples_per_second': 31.549, 'eval_steps_per_second': 4.101, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umeleti/.conda/envs/pytorch-p100/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /scratch/umeleti/code/LLM/Text2SQL/MODEL_CACHE/NousResearch--CodeLlama-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0919, 'learning_rate': 8.888888888888888e-06, 'epoch': 0.02}\n",
      "{'eval_loss': 1.0054242610931396, 'eval_runtime': 3.1699, 'eval_samples_per_second': 31.547, 'eval_steps_per_second': 4.101, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umeleti/.conda/envs/pytorch-p100/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /scratch/umeleti/code/LLM/Text2SQL/MODEL_CACHE/NousResearch--CodeLlama-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8893, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.03}\n",
      "{'eval_loss': 0.9247572422027588, 'eval_runtime': 3.17, 'eval_samples_per_second': 31.545, 'eval_steps_per_second': 4.101, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umeleti/.conda/envs/pytorch-p100/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /scratch/umeleti/code/LLM/Text2SQL/MODEL_CACHE/NousResearch--CodeLlama-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8559, 'learning_rate': 4.444444444444444e-06, 'epoch': 0.03}\n",
      "{'eval_loss': 0.8862056732177734, 'eval_runtime': 3.1705, 'eval_samples_per_second': 31.541, 'eval_steps_per_second': 4.1, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umeleti/.conda/envs/pytorch-p100/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /scratch/umeleti/code/LLM/Text2SQL/MODEL_CACHE/NousResearch--CodeLlama-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8649, 'learning_rate': 2.222222222222222e-06, 'epoch': 0.04}\n",
      "{'eval_loss': 0.8640143871307373, 'eval_runtime': 3.1718, 'eval_samples_per_second': 31.528, 'eval_steps_per_second': 4.099, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umeleti/.conda/envs/pytorch-p100/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /scratch/umeleti/code/LLM/Text2SQL/MODEL_CACHE/NousResearch--CodeLlama-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7085, 'learning_rate': 0.0, 'epoch': 0.04}\n",
      "{'eval_loss': 0.8583202958106995, 'eval_runtime': 3.1722, 'eval_samples_per_second': 31.524, 'eval_steps_per_second': 4.098, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umeleti/.conda/envs/pytorch-p100/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /scratch/umeleti/code/LLM/Text2SQL/MODEL_CACHE/NousResearch--CodeLlama-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 424.0208, 'train_samples_per_second': 7.547, 'train_steps_per_second': 0.472, 'train_loss': 1.3131573629379272, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umeleti/.conda/envs/pytorch-p100/lib/python3.9/site-packages/peft/utils/save_and_load.py:148: UserWarning: Could not find a config file in /scratch/umeleti/code/LLM/Text2SQL/MODEL_CACHE/NousResearch--CodeLlama-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bec2bd1c-52c0-4975-b0e2-37cf5fc1fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data_points, add_eos_token=True, train_on_inputs=False, cutoff_len=512\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Tokenizes dataset of SQL related data points consisting of questions, context, and answers.\n",
    "\n",
    "        Parameters:\n",
    "            data_points (dict): A batch from the dataset containing 'question', 'context', and 'answer'.\n",
    "            add_eos_token (bool): Whether to add an EOS token at the end of each tokenized sequence.\n",
    "            cutoff_len (int): The maximum length for each tokenized sequence.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing tokenized 'input_ids', 'attention_mask', and 'labels'.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            question = data_points[\"question\"]\n",
    "            context = data_points[\"context\"]\n",
    "            answer = data_points[\"answer\"]\n",
    "            if train_on_inputs:\n",
    "                user_prompt = generate_prompt_sql(question, context)\n",
    "                tokenized_user_prompt = tokenizer(\n",
    "                    user_prompt,\n",
    "                    truncation=True,\n",
    "                    max_length=cutoff_len,\n",
    "                    padding=False,\n",
    "                    return_tensors=None,\n",
    "                )\n",
    "                user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "                if add_eos_token:\n",
    "                    user_prompt_len -= 1\n",
    "\n",
    "            combined_text = generate_prompt_sql(question, context, answer)\n",
    "            tokenized = tokenizer(\n",
    "                combined_text,\n",
    "                truncation=True,\n",
    "                max_length=cutoff_len,\n",
    "                padding=False,\n",
    "                return_tensors=None)\n",
    "            if (tokenized[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "                and add_eos_token\n",
    "                and len(tokenized[\"input_ids\"]) < cutoff_len):\n",
    "                \n",
    "                tokenized[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "                tokenized[\"attention_mask\"].append(1)\n",
    "                \n",
    "            tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "            \n",
    "            if train_on_inputs:\n",
    "                tokenized[\"labels\"] = [-100] * user_prompt_len + tokenized[\"labels\"][\n",
    "                    user_prompt_len:\n",
    "                ]\n",
    "\n",
    "            return tokenized\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                f\"Error in batch tokenization: {e}, Line: {e.__traceback__.tb_lineno}\"\n",
    "            )\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3f5a8-d040-49f5-bf70-b39fc88d6a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-p100",
   "language": "python",
   "name": "pytorch-p100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
